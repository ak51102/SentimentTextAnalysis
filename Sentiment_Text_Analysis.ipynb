{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Text Analysis.ipynb",
      "provenance": [],
      "private_outputs": true,
      "mount_file_id": "1aidmupgYxzzx8z-mYF9gO-zivBJUsMBS",
      "authorship_tag": "ABX9TyNc6gaCvVqbxPKiGbRxaU7C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/SentimentTextAnalysis/blob/master/Sentiment_Text_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43caXH-JktTN",
        "colab_type": "text"
      },
      "source": [
        "# **Pre-requisites:**<br>\n",
        "Python - 2 day course is sufficient<br>\n",
        "Keras understanding - intro course is sufficient<br>\n",
        "logistic regression - understanding<br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP-R61seyu6s",
        "colab_type": "text"
      },
      "source": [
        "# **Pre-work**\n",
        "You will need to add a file to your google drive. <br>\n",
        "1. Download the file to your computer. \n",
        ">Click on the link below. Then click **Download** <br>\n",
        "The download can take as long as 15 minutes.<br>\n",
        "The file to download is: [fileToAddToGoogleDrive](https://drive.google.com/open?id=1zJI1Xz-CgaQqX1UtBcOhUjEKWcSt6QK6)<br>\n",
        "The file is large: 2GBytes<br><br>\n",
        "\n",
        "\n",
        "2. Upload the file to Google Drive:<br>\n",
        ">Open Google Drive<br>\n",
        "On the Drive menu, click on **New** >> **File Upload**<br>\n",
        "Find the file on your computer, click on it and upload the file. \n",
        "\n",
        "The file is large, it may take as long as 15 minutes<br>\n",
        "Once the file is on your Google Drive, you can delete it from your computer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4pcoAN9DPsI",
        "colab_type": "text"
      },
      "source": [
        "The file is from a website: [English word vectors](https://fasttext.cc/docs/en/english-vectors.html)<br>\n",
        "This page gathers several pre-trained word vectors trained using fastText."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GUmmVaSCDTV",
        "colab_type": "text"
      },
      "source": [
        "# **Mount your Google Drive on this CoLab Notebook**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-0Y2vgPCDfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4fFT8ZFeTDT",
        "colab_type": "text"
      },
      "source": [
        "Using scikit-learn library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPxnK5iGdOO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/cagBRT/SentimentTextAnalysis.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTXaGN8jfvT3",
        "colab_type": "text"
      },
      "source": [
        "# **Import the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6UfoWoDombL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ_ITPKgr0xj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caQfFnw6lU6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs6VDuowfVAg",
        "colab_type": "text"
      },
      "source": [
        "# **Examine the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TlFVDiLdnIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!cat yelp_labelled.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z3rmOeecdQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath_dict = {'yelp':   'yelp_labelled.txt',\n",
        "                 'amazon': 'amazon_cells_labelled.txt',\n",
        "                 'imdb':   'imdb_labelled.txt'}\n",
        "\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "    df['source'] = source  # Add another column filled with the source name\n",
        "    df_list.append(df)\n",
        "\n",
        "df = pd.concat(df_list)\n",
        "print(df.iloc[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otrPFs_FfJOB",
        "colab_type": "text"
      },
      "source": [
        "# **Create a Bag of Words**\n",
        "Create a bag of words (BoW) for vectorizing the text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7VqhzIZd_hG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "john_words = ['John likes to run.', 'John hates to be cold.', 'John hates to be late.']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5UPXWFMeFTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
        "vectorizer.fit(john_words)\n",
        "vectorizer.vocabulary_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM-AeTLVewzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "vectorizer.transform(john_words).toarray()\n",
        "\n",
        "dfbow = pd.DataFrame()\n",
        "dfbow['voc']= vectorizer.vocabulary_\n",
        "dfbow.sort_values(by=['voc'])\n",
        "cat_columns = [\"voc\"]\n",
        "df_processed = pd.get_dummies(dfbow, prefix_sep=\"__\",columns=cat_columns)\n",
        "df_processed\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPM1uU2mf-15",
        "colab_type": "text"
      },
      "source": [
        "# **Split the data into train and test sets**\n",
        "\n",
        "Split the Yelp data into training and tests sets<br>\n",
        "\n",
        "[train_test_split](https://www.bitdegree.org/learn/train-test-split)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6jCeLe9f8pp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_yelp = df[df['source'] == 'yelp']\n",
        "\n",
        "sentences = df_yelp['sentence'].values\n",
        "y = df_yelp['label'].values\n",
        "\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
        "   sentences, y, test_size=0.25, random_state=1000)\n",
        "print(sentences_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT0D7qx5gPtD",
        "colab_type": "text"
      },
      "source": [
        "# **Vectorize the training and test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrXmpO--gP4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(sentences_train)\n",
        "\n",
        "X_train = vectorizer.transform(sentences_train)\n",
        "X_test  = vectorizer.transform(sentences_test)\n",
        "\n",
        "check=0\n",
        "print(sentences_train[check])\n",
        "print(X_train[check])\n",
        "#Prints sentence number, word vector, quantity of word in sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svvE2Vfogj4Z",
        "colab_type": "text"
      },
      "source": [
        "**The training set has:** <br>\n",
        "750 examples<br>\n",
        "1714 words in the vocabulary<br>\n",
        "\n",
        "It is a sparse matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnW6SK_piqQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ8WySaNjHQg",
        "colab_type": "text"
      },
      "source": [
        "# **Create a logistic regression model and train it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXYDqEdrjHar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n",
        "score = classifier.score(X_test, y_test)\n",
        "\n",
        "print(\"Accuracy:\", score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4tRFkQBjhR1",
        "colab_type": "text"
      },
      "source": [
        "# **Baseline: Perform logistic regression on all three data sets**<br>\n",
        "yelp<br>\n",
        "amazon<br>\n",
        "imdb<br>\n",
        "\n",
        "Get a baseline using logistic regression. This will give us something to compare with the other methods. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y74YPIkbjYMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for source in df['source'].unique():\n",
        "    df_source = df[df['source'] == source]\n",
        "    reviews = df_source['sentence'].values\n",
        "    reviews_y = df_source['label'].values\n",
        "\n",
        "    reviews_train, reviews_test, reviews_y_train, reviews_y_test = train_test_split(\n",
        "        reviews, reviews_y, test_size=0.25, random_state=1000)\n",
        "\n",
        "    vectorizer = CountVectorizer()\n",
        "    vectorizer.fit(reviews_train)\n",
        "    r_X_train = vectorizer.transform(reviews_train)\n",
        "    r_X_test  = vectorizer.transform(reviews_test)\n",
        "\n",
        "    classifier = LogisticRegression()\n",
        "    classifier.fit(r_X_train, reviews_y_train)\n",
        "    score = classifier.score(r_X_test, reviews_y_test)\n",
        "    print('Accuracy for {} data: {:.4f}'.format(source, score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWkguVMWlc3b",
        "colab_type": "text"
      },
      "source": [
        "# **Trial 1:Keras DNN**\n",
        "Create a DNN using Keras. \n",
        "Compare it to the logistic regession using the same data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl_GA_keldAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = X_train.shape[1]  # Number of features\n",
        "print(\"model imputs = \", input_dim)\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(2500, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(1000, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(1000, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpfbM8Plr_6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MClsUloHsfeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=20,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNfZaUNtssWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4YFCVmuszj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRNL-Xsls22X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x6ZEW6238GM",
        "colab_type": "text"
      },
      "source": [
        "# **Word Embedding**\n",
        "There are various ways to vectorize text, such as:\n",
        "*   Words represented as a vector.\n",
        "*   Characters represented as a vector\n",
        "\n",
        "\n",
        "In this notebook, you’ll see how to deal with representing words as vectors which is the common way to use text in neural networks. Two possible ways to represent a word as a vector are:\n",
        "*   one-hot encoding\n",
        "*   Lword embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocESp_nW4Nff",
        "colab_type": "text"
      },
      "source": [
        "**Hot-one encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvmlZhoC3-X5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cities = ['London', 'Berlin', 'Berlin', 'New York', 'London']\n",
        "cities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avkg8U2t4VWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "city_labels = encoder.fit_transform(cities)\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['cities']= cities\n",
        "df['city_labels']= city_labels\n",
        "df.sort_values(by=['cities'])\n",
        "\n",
        "cat_columns = [\"city_labels\"]\n",
        "df_processed = pd.get_dummies(df, prefix_sep=\"__\",columns=cat_columns)\n",
        "df_processed\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIzHxBvk4mez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "city_labels = city_labels.reshape((5, 1))\n",
        "encoder.fit_transform(city_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkYyfbnJjPS_",
        "colab_type": "text"
      },
      "source": [
        "**Word embedding**<br>\n",
        "Word embedding has fewer dimensions than one-hot encoding<br>\n",
        "Word embedding places similar words near each other<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE096qUiyYPM",
        "colab_type": "text"
      },
      "source": [
        "This method represents words as dense word vectors (also called word embeddings) which are trained unlike the one-hot encoding which are hardcoded. This means that the word embeddings collect more information into fewer dimensions.\n",
        "\n",
        "Note that the word embeddings do not understand the text as a human would, but they rather map the statistical structure of the language used in the corpus. Their aim is to map semantic meaning into a geometric space. This geometric space is then called the embedding space.<br>\n",
        "\n",
        "This would map semantically similar words close on the embedding space like numbers or colors. If the embedding captures the relationship between words well, things like vector arithmetic should become possible. A famous example in this field of study is the ability to map King - Man + Woman = Queen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9EAi8ZEyjnH",
        "colab_type": "text"
      },
      "source": [
        "How can you get such a word embedding? <br>\n",
        "You have two options for this. \n",
        "\n",
        ">1.Train your word embeddings during the training of your neural network. <br>\n",
        ">2.Use pretrained word embeddings which you can directly use in your model. You can leave these word embeddings unchanged during training or you can train them.<br><br>\n",
        "\n",
        "Now you need to tokenize the data into a format that can be used by the word embeddings. <br><br>\n",
        "Keras offers a couple of convenience methods for text preprocessing and sequence preprocessing which you can employ to prepare your text.<br>\n",
        "\n",
        "[Keras Tokenizer ](https://keras.io/preprocessing/text/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjmlLvVCyZF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=3000) #keep 3000 words\n",
        "\n",
        "#Updates internal vocabulary based on a list of texts\n",
        "#Must be run before running texts_to_sequences\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "\n",
        "print(\"vocab size=\", vocab_size)\n",
        "number = 0\n",
        "print(sentences_train[number])\n",
        "print(X_train[number])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXBx3rLC0IQb",
        "colab_type": "text"
      },
      "source": [
        "The indexing begins with the most common word first (the). <br>\n",
        "It is important to note that the index 0 is reserved and is not assigned to any word. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDkVfwDTzm0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Entering a word that is not in the texts will\n",
        "#generate an error\n",
        "for word in ['the', 'all', 'bad', 'terrible','horrible','lost','lukewarm','bacon']: \n",
        "    print('{}: {}'.format(word, tokenizer.word_index[word]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgJBNUDVtmlH",
        "colab_type": "text"
      },
      "source": [
        "**Find similar words with gensim**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne2MZqlUbjzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim.downloader as api\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
        "result = word_vectors.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])\n",
        "print(\"{}: {:.4f}\".format(*result[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPB6nRbJkUgQ",
        "colab_type": "text"
      },
      "source": [
        "**Pad the sequence of words**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hZnB5SD1E82",
        "colab_type": "text"
      },
      "source": [
        "One problem that we have is that each text sequence has in most cases different length of words. To counter this, you can use pad_sequence() which simply pads the sequence of words with zeros. By default, it prepends zeros but we want to append them. Typically it does not matter whether you prepend or append zeros.\n",
        "\n",
        "Additionally you would want to add a maxlen parameter to specify how long the sequences should be. This cuts sequences that exceed that number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSmvBgUk1yRg",
        "colab_type": "text"
      },
      "source": [
        "The resulting feature vector contains mostly zeros, when you have a fairly short sentence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25FkOsNL1Jl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "#The maximum length of a review \n",
        "maxlen = 100\n",
        "#If a review is less than 100 words, pad the vector with 0s.\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "print(X_train.shape,X_test.shape)\n",
        "print(y_train.shape,y_test.shape)\n",
        "\n",
        "index=0\n",
        "print(sentences_train[index])\n",
        "print(X_train[index, :])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ImkmiOQ1632",
        "colab_type": "text"
      },
      "source": [
        "Now you can use the Embedding Layer of Keras which takes the previously calculated integers and maps them to a dense vector of the embedding. <br>\n",
        "You will need the following parameters:<br>\n",
        "\n",
        ">input_dim: the size of the vocabulary<br>\n",
        "output_dim: the size of the dense vector<br>\n",
        "input_length: the length of the sequence<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_A0dS0p17hu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"input dim=\",input_dim)\n",
        "print(\"output dim of embedding layer=\",embedding_dim)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuZ3mihe3I1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_train.shape,X_test.shape)\n",
        "print(y_train.shape,y_test.shape)\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdnxohmj4Bsy",
        "colab_type": "text"
      },
      "source": [
        "This is typically a not very reliable way to work with sequential data as you can see in the performance. When working with sequential data you want to focus on methods that look at local and sequential information instead of absolute positional information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxV9Z7lU4KDb",
        "colab_type": "text"
      },
      "source": [
        "Another way to work with embeddings is by using a MaxPooling1D/AveragePooling1D or a GlobalMaxPooling1D/GlobalAveragePooling1D layer after the embedding. You can think of the pooling layers as a way to downsample (a way to reduce the size of) the incoming feature vectors.\n",
        "\n",
        "In the case of max pooling you take the maximum value of all features in the pool for each feature dimension. In the case of average pooling you take the average, but max pooling seems to be more commonly used as it highlights large values.\n",
        "\n",
        "Global max/average pooling takes the maximum/average of all features whereas in the other case you have to define the pool size. Keras has again its own layer that you can add in the sequential model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvcohC3N4NCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MU5Xh1l4Tjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1S2XEG15FNr",
        "colab_type": "text"
      },
      "source": [
        "use a precomputed embedding space that utilizes a much larger corpus. It is possible to precompute word embeddings by simply training them on a large corpus of text. Among the most popular methods are Word2Vec developed by Google and GloVe (Global Vectors for Word Representation) developed by the Stanford NLP Group.<br>\n",
        "\n",
        "Word2Vec achieves this by employing neural networks and GloVe achieves this with a co-occurrence matrix and by using matrix factorization. In both cases you are dealing with dimensionality reduction, but Word2Vec is more accurate and GloVe is faster to compute.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDHM_LB98qW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-MvYxlUBEso",
        "colab_type": "text"
      },
      "source": [
        "# **CHECK: DOES THE FOLLOWING CODE CELL EXECUTE WITHOUT ERRORS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yg52fWY8vrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 50\n",
        "embedding_matrix = create_embedding_matrix(\n",
        "    '/content/drive/My Drive/ZUp/wiki-news-300d-1M.vec',\n",
        "    tokenizer.word_index, embedding_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zrIyzaHZmHC",
        "colab_type": "text"
      },
      "source": [
        "Percentage of vocabulary covered by the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zerTRtUWZhbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
        "nonzero_elements / vocab_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVbEXqtepJk0",
        "colab_type": "text"
      },
      "source": [
        "# **Trial 3: Embedded DNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqjXj7jZZts8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=maxlen, \n",
        "                           trainable=False))\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRcByh6-Zycf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTgLVaxeZ8S5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=maxlen, \n",
        "                           trainable=True))\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rgHBRNDaAYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT3MaMvPacWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVPXIiu8agJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os0yrNZaapZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
        "    model = Sequential()\n",
        "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(10, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',metrics=[\"acc\"])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLEXBJP4ktau",
        "colab_type": "text"
      },
      "source": [
        "**Embedding dimension**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLd-_2jQasgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_grid = dict(num_filters=[32, 64, 128],\n",
        "                  kernel_size=[3, 5, 7],\n",
        "                  vocab_size=[5000], \n",
        "                  embedding_dim=[50],\n",
        "                  maxlen=[100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dW84sk_fdT2",
        "colab_type": "text"
      },
      "source": [
        "# **HyperParameter Grid Search of each text set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoQUA3AVBjAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath_dict = {'yelp':   'yelp_labelled.txt',\n",
        "                 'amazon': 'amazon_cells_labelled.txt',\n",
        "                 'imdb':   'imdb_labelled.txt'}\n",
        "\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "    df['source'] = source  # Add another column filled with the source name\n",
        "    df_list.append(df)\n",
        "\n",
        "df = pd.concat(df_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlkYfUuKaww4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Main settings\n",
        "epochs = 20\n",
        "embedding_dim = 50\n",
        "maxlen = 100\n",
        "output_file = '/content/drive/My Drive/output.txt'\n",
        "\n",
        "# Run grid search for each source (yelp, amazon, imdb)\n",
        "for source, frame in df.groupby('source'):\n",
        "    print('Running grid search for data set :', source)\n",
        "    sentences = df['sentence'].values\n",
        "    y = df['label'].values\n",
        "\n",
        "    # Train-test split\n",
        "    sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
        "        sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "    # Tokenize words\n",
        "    tokenizer = Tokenizer(num_words=5000)\n",
        "    tokenizer.fit_on_texts(sentences_train)\n",
        "    X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "    X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "    # Adding 1 because of reserved 0 index\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "    # Pad sequences with zeros\n",
        "    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "    X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "    # Parameter grid for grid search\n",
        "    param_grid = dict(num_filters=[32, 64, 128],\n",
        "                      kernel_size=[3, 5, 7],\n",
        "                      vocab_size=[vocab_size],\n",
        "                      embedding_dim=[embedding_dim],\n",
        "                      maxlen=[maxlen])\n",
        "    model = KerasClassifier(build_fn=create_model,\n",
        "                            epochs=epochs, batch_size=10,\n",
        "                            verbose=False)\n",
        "    grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
        "                              cv=4, verbose=1, n_iter=5)\n",
        "    grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate testing set\n",
        "    test_accuracy = grid.score(X_test, y_test)\n",
        "\n",
        "    # Save and evaluate results\n",
        "    #prompt = input(f'finished {source}; write to file and proceed? [y/n]')\n",
        "    #if prompt.lower() not in {'y', 'true', 'yes'}:\n",
        "    #    break\n",
        "    with open(output_file, 'a') as f:\n",
        "        s = ('Running {} data set\\nBest Accuracy : '\n",
        "             '{:.4f}\\n{}\\nTest Accuracy : {:.4f}\\n\\n')\n",
        "        output_string = s.format(\n",
        "            source,\n",
        "            grid_result.best_score_,\n",
        "            grid_result.best_params_,\n",
        "            test_accuracy)\n",
        "        print(output_string)\n",
        "        f.write(output_string)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}