{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Text Analysis 2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "mount_file_id": "1aidmupgYxzzx8z-mYF9gO-zivBJUsMBS",
      "authorship_tag": "ABX9TyPk7lx13yyKfoCs4JRDY543",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/SentimentTextAnalysis/blob/master/Sentiment_Text_Analysis_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPxnK5iGdOO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone  https://github.com/cagBRT/SentimentTextAnalysis.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m_-f0D5AUSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "def page(num):\n",
        "    return Image(\"images/sentTextAna\"+str(num)+ \".png\" , width=600)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43caXH-JktTN",
        "colab_type": "text"
      },
      "source": [
        "# **Pre-requisites:**<br>\n",
        "Python - 2 day course is sufficient<br>\n",
        "Keras understanding - intro course is sufficient<br>\n",
        "logistic regression - understanding<br>\n",
        "Deep neural networks - intro course is sufficient<br>\n",
        "Hyperparameter tuning _ intro course is sufficient<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP-R61seyu6s",
        "colab_type": "text"
      },
      "source": [
        "# **Pre-work**\n",
        "**Do this step before coming to class. It may take as long as 30 minutes to complete the download and upload.**<br>\n",
        "<br>\n",
        "You will need to add a file to your google drive. <br>\n",
        "1. Download the file to your computer. \n",
        ">Click on the link below. Then click **Download** <br>\n",
        "The download can take as long as 15 minutes.<br>\n",
        "The file to download is: [fileToAddToGoogleDrive](https://drive.google.com/open?id=1zJI1Xz-CgaQqX1UtBcOhUjEKWcSt6QK6)<br>\n",
        "The file is large: 2GBytes<br><br>\n",
        "\n",
        "\n",
        "2. Upload the file to Google Drive:<br>\n",
        ">Open Google Drive<br>\n",
        "On the Drive menu, click on **New** >> **File Upload**<br>\n",
        "Find the file on your computer, click on it and upload the file. \n",
        "\n",
        "The file is large, it may take as long as 15 minutes<br>\n",
        "Once the file is on you<br><br>\n",
        "The file is from a website: [English word vectors](https://fasttext.cc/docs/en/english-vectors.html)<br>\n",
        "This page gathers several pre-trained word vectors trained using fastText."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a68Nq4HgJDF",
        "colab_type": "text"
      },
      "source": [
        "# **Sentiment Text Analyis**<br>\n",
        "This notebook introduces basic machine learning methods for determing if yelp, Amazon, and imdb reviews are positive or negative. <br>\n",
        "The following topics are covered in this notebook: \n",
        "1. Create and train a baseline model.\n",
        "2. Create and train a simple DNN model\n",
        "3. Add an embedded layer to a simple DNN model\n",
        "4. Add a MaxPooling layer to an embedded model\n",
        "5. Use a pretrained embedded model\n",
        "6. Create a CNN model with an embedded layer\n",
        "7. Do hyperparameter tuning using random search\n",
        "\n",
        "It does not cover recurrent neural networks or LSTM or GRU. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoYY6S0GBIYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "page(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfsq6tt92qyj",
        "colab_type": "text"
      },
      "source": [
        "**What is sentiment analysis?**<br>\n",
        "Sentiment analysis is a branch of natural language processing. It is one of the most important sources in decision making for determining the sentiment of language. It can extract from, identify, evaluate, or characterize online sentiment of reviews.<br>\n",
        "\n",
        "Sentiment analysis assumes various forms, from models that focus on polarity (positive, negative, neutral) to those that detect feelings and emotions (angry, happy, sad, etc), or even models that identify intentions (e.g. interested v. not interested).<br><br>\n",
        "\n",
        "It’s estimated that 80% of the world’s data is unstructured, in other words it’s unorganized. Huge amounts of text data (emails, support tickets, chats, social media conversations, surveys, articles, documents, etc), is created every day but it’s hard to analyze, understand, and sort through, not to mention time-consuming and expensive.\n",
        "\n",
        "Sentiment analysis, however, helps businesses make sense of all this unstructured text by automatically tagging it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qpr1ZIVa47D_",
        "colab_type": "text"
      },
      "source": [
        "**Benefits of sentiment analysis**<br>\n",
        "Benefits of sentiment analysis include:\n",
        "\n",
        "1. Processes data at scale: Sentiment analysis helps businesses process huge amounts of data in an efficient and cost-effective way.\n",
        "\n",
        "2. Real-Time analysis: Sentiment analysis can identify critical issues in real-time, for example is a PR crisis on social media escalating? Is an angry customer about to churn? Sentiment analysis models can  immediately identify, through text, these types of situations. \n",
        "\n",
        "3. Consistent criteria: Tagging text by sentiment is highly subjective, influenced by personal experiences, thoughts, and beliefs. It’s estimated that people only agree around 60-65% of the time when determining the sentiment of a particular text. By using a centralized sentiment analysis system, companies can apply the same criteria to all of their data, helping them improve accuracy and gain better insights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GUmmVaSCDTV",
        "colab_type": "text"
      },
      "source": [
        "# **Mount your Google Drive on this CoLab Notebook**\n",
        "Execute the following code cell<br>\n",
        "Click on the given link<br>\n",
        "Select your user name<br>\n",
        "Click **Allow**<br>\n",
        "Copy the authorization code<br>\n",
        "Paste the authorization code into the user input box. <br>\n",
        "You Google Drive is mounted to this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-0Y2vgPCDfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C_h3O7HD08K",
        "colab_type": "text"
      },
      "source": [
        "# **Check that your drive is mounted**\n",
        "1. On the menu bar, click the **folder icon**<br>\n",
        "2. Click on the **folder icon with the up arrow**\n",
        "3. Click on **gdrive**\n",
        "4. Click on **My Drive**\n",
        "5. Check for the file called **wiki-news-300d-1M.vec **<br>\n",
        "If the file is there, you have correctly installed the necessary files for this notebook. <br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4fFT8ZFeTDT",
        "colab_type": "text"
      },
      "source": [
        "# **scikit-learn**\n",
        "This notebook uses the [scikit-learn](https://scikit-learn.org/stable/) library<br>\n",
        "Scikit-learn is a free software machine learning library for Python.<br>\n",
        "*   Simple and efficient tools for predictive data analysis\n",
        "*  Accessible to everybody, and reusable in various contexts\n",
        "*Built on NumPy, SciPy, and matplotlib\n",
        "*Open source, commercially usable - BSD license\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTXaGN8jfvT3",
        "colab_type": "text"
      },
      "source": [
        "# **Import the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6UfoWoDombL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ_ITPKgr0xj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caQfFnw6lU6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs6VDuowfVAg",
        "colab_type": "text"
      },
      "source": [
        "# **Examine the data**<br>\n",
        "The data is from three sources: <br>\n",
        "> yelp reviews<br>\n",
        "> amazon reviews<br>\n",
        "> movie reviews<br>\n",
        "\n",
        "The data has the structure: <br>\n",
        ">\"review text\" label source<br>\n",
        "\n",
        "**review text is called**: sentence<br>\n",
        "**label**: 0 = negative review, 1 = positive review<br>\n",
        "**source**: yelp, amazon, imdb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TlFVDiLdnIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!cat yelp_labelled.txt\n",
        "#Change directory to the cloned repo\n",
        "%cd /content/cloned-repo/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z3rmOeecdQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a dataframe containing all three sources\n",
        "filepath_dict = {'yelp':   'yelp_labelled.txt',\n",
        "                 'amazon': 'amazon_cells_labelled.txt',\n",
        "                 'imdb':   'imdb_labelled.txt'}\n",
        "\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "    df['source'] = source  # Add another column filled with the source name\n",
        "    df_list.append(df)\n",
        "\n",
        "df = pd.concat(df_list)\n",
        "print(df.iloc[0])\n",
        "print(\"dataframe shape: \",df.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otrPFs_FfJOB",
        "colab_type": "text"
      },
      "source": [
        "# **Create a Bag of Words**\n",
        "The bag-of-words model is a word/sentence representation used in NLP and information retrieval. <br>\n",
        "In this model, text is represented as a bag of its words, *disregarding grammar and even word order*.\n",
        "\n",
        "Create a bag of words (BoW) for vectorizing the text. <br>\n",
        "Take the data and create a vocabulary from all the words in all reviews.<br>\n",
        "The collection of texts is also called a **corpus** in NLP.<br>\n",
        "The **vocabulary** in this case is a list of words that occurred in our text where each word has its own index.<br>\n",
        "The resulting vector is also called a **feature vector**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM4CfF-1O7fQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "page(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSzPfHZZhHJS",
        "colab_type": "text"
      },
      "source": [
        "# **Bag of Words Example:** <br>\n",
        "# **Create a bag of words (BoW) for vectorizing the text**\n",
        "\n",
        "Create a toy set of words to demonstrate BoW.<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7VqhzIZd_hG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1. The words\n",
        "john_words = ['Sam likes to run.', \n",
        "              'John hates to be cold and he hates to run.', \n",
        "              'John hates to be late.'\n",
        "            ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5UPXWFMeFTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2. Create the bag of words\n",
        "#The words are in alphabetical order (uppercase listed before lowercase)\n",
        "#Each word is assigned a number\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
        "vectorizer.fit(john_words)\n",
        "vocab = vectorizer.vocabulary_\n",
        "vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM-AeTLVewzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#3. Convert the sentences in to vectors\n",
        "#each word used in a sentence has a '1' in the corresponding column\n",
        "dfbow = pd.DataFrame()\n",
        "dfbow['sentences'] = john_words\n",
        "vector = vectorizer.transform(john_words).toarray().tolist()\n",
        "dfbow['vector'] = vector\n",
        "print(vocab)\n",
        "dfbow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_obQxnIey13Q",
        "colab_type": "text"
      },
      "source": [
        "Notice in the above output: \n",
        "1. All the words in all the sentences created the vocabulary.<br>\n",
        "2. Each sentence is turned into a vector. <br>\n",
        "3. The length of the vector is the length of the vocabulary.<br>\n",
        "4. A '1' in the vector indicates the word is used in the sentence, a '0' means it is not used in the sentence. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vHNtHmOL0lQ",
        "colab_type": "text"
      },
      "source": [
        "# **Assignment 1**: <br>\n",
        "1. Write 5 sentences. \n",
        "2. Create a BoW from the sentences. \n",
        "3. Vectorize the sentences. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeh-7rsK2chJ",
        "colab_type": "text"
      },
      "source": [
        "Although Bag-Of-Words model is the most widely used technique for sentiment analysis, it has two major weaknesses: <br>\n",
        "* the larger the vocabulary, the larger the feature vector\n",
        "* it assumes each word is independent of other words\n",
        "* the feature vectors are extremely sparse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0m107q4SwNq",
        "colab_type": "text"
      },
      "source": [
        "A **sparse matrix**  is a matrix in which most of the elements are zero<br>\n",
        "A **dense matrix**  is a matrix in which most of the elements are nonzero\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPM1uU2mf-15",
        "colab_type": "text"
      },
      "source": [
        "# **Split the review data into train and test sets**\n",
        "\n",
        "Split the Yelp data into training and tests sets<br>\n",
        "\n",
        "[train_test_split](https://www.bitdegree.org/learn/train-test-split)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6jCeLe9f8pp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#select the rows of the data set that are from yelp\n",
        "df_yelp = df[df['source'] == 'yelp']\n",
        "\n",
        "sentences = df_yelp['sentence'].values\n",
        "y = df_yelp['label'].values\n",
        "\n",
        "#do a 75 - 25 split between train and test data\n",
        "#If int, random_state is the seed used by the random number generator; \n",
        "#If RandomState instance, random_state is the random number generator; \n",
        "#If None, the random number generator is the RandomState instance used by np.random.\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
        "   sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "#print out the first sentence of the training set\n",
        "print(sentences_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT0D7qx5gPtD",
        "colab_type": "text"
      },
      "source": [
        "# **Vectorize the training and test set**\n",
        "Vectorize the data: <br>\n",
        "\n",
        "Assign each word a number.<br>\n",
        "Count the number of times each word appears in the individual review text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrXmpO--gP4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#1. use the words from the training set\n",
        "#2. create a BoW from the yelp reviews\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(sentences_train)\n",
        "vocab = vectorizer.vocabulary_\n",
        "vocab = pd.Series(vocab)\n",
        "#2. vectorize the sentences\n",
        "X_train = vectorizer.transform(sentences_train)\n",
        "X_test  = vectorizer.transform(sentences_test)\n",
        "print(\"training data: \", X_train.shape,\"\\ntest data: \", X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE6MTaQNJXh5",
        "colab_type": "text"
      },
      "source": [
        "What has been done so far: \n",
        "1. Created a vocabulary from all the words used in the yelp reviews.\n",
        "2. Assigned each word a number.<br>\n",
        "\n",
        "Now check the vectorization of the sentences in the yelp review training and test data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GFHSK8qJDAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\"Select a number between 0 - 749\n",
        "check=24\n",
        "print(sentences_train[check])\n",
        "print(X_train[check])\n",
        "#Prints sentence number, word vector, quantity of word in sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIn6nF381OLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\"Select a number between 0 - 749\n",
        "check=24\n",
        "print(sentences_test[check])\n",
        "print(X_test[check])\n",
        "#Prints sentence number, word vector, quantity of word in sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRTOQFRqMdbX",
        "colab_type": "text"
      },
      "source": [
        "# **Assignment #2**: \n",
        "1. Split the Amazon reviews into a training and test set.\n",
        "2. Create a BoW using the amazon training and test reviews. \n",
        "2. Print out the vectorization of two or three reviews from the test and training set. <br>\n",
        "**Make sure your variable names for the Amazon BoW are different than the Yelp BoW.** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ8WySaNjHQg",
        "colab_type": "text"
      },
      "source": [
        "# **Create a logistic regression model and train it**<br>\n",
        "\n",
        "The [Logistic Regression class](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) from Scikit Learn implements regularized logistic regression. It can handle both dense and sparse input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXYDqEdrjHar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n",
        "score = classifier.score(X_test, y_test)\n",
        "\n",
        "print(\"Accuracy:\", score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4tRFkQBjhR1",
        "colab_type": "text"
      },
      "source": [
        "# **Assignment 3:**\n",
        "\n",
        "Perform logistic regression on the other two data sets<br>\n",
        ">amazon<br>\n",
        "imdb<br>\n",
        "\n",
        "Get a baseline using logistic regression. This will give us something to compare with the other methods. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y74YPIkbjYMu",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title \n",
        "for source in df['source'].unique():\n",
        "    df_source = df[df['source'] == source]\n",
        "    reviews = df_source['sentence'].values\n",
        "    reviews_y = df_source['label'].values\n",
        "\n",
        "    reviews_train, reviews_test, reviews_y_train, reviews_y_test = train_test_split(\n",
        "        reviews, reviews_y, test_size=0.25, random_state=1000)\n",
        "\n",
        "    vectorizer = CountVectorizer()\n",
        "    vectorizer.fit(reviews_train)\n",
        "    r_X_train = vectorizer.transform(reviews_train)\n",
        "    r_X_test  = vectorizer.transform(reviews_test)\n",
        "\n",
        "    classifier = LogisticRegression()\n",
        "    classifier.fit(r_X_train, reviews_y_train)\n",
        "    score = classifier.score(r_X_test, reviews_y_test)\n",
        "    print('Accuracy for {} data: {:.4f}'.format(source, score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWkguVMWlc3b",
        "colab_type": "text"
      },
      "source": [
        "# **Trial 1:Keras DNN**\n",
        "Create a DNN using Keras, use the Yelp reviews bag of words.<br> \n",
        "Compare it to the logistic regession using the same data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRZwkRe4T3Ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "page(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl_GA_keldAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = X_train.shape[1]  # Number of features\n",
        "print(\"model imputs = \", input_dim)\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(1700, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(1000, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2HnnVKQUdVF",
        "colab_type": "text"
      },
      "source": [
        "# Discussion:\n",
        "Given the dataset and the model, what do you expect to happen? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpfbM8Plr_6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG0lvs-h7CkW",
        "colab_type": "text"
      },
      "source": [
        "**Train the DNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MClsUloHsfeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNfZaUNtssWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4YFCVmuszj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geAPUYZE-Ddt",
        "colab_type": "text"
      },
      "source": [
        "The Deep Neural Network trained using Bag of Words is overfit. <br>\n",
        "Play with the model architecture and hyperparameters to see if you can find a better model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRNL-Xsls22X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdMV7FP8-Yob",
        "colab_type": "text"
      },
      "source": [
        "In the BOW model, you  represented an entire review as a single feature vector. In the next section, each word is represented as a vector. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM3Y-3IsPKNA",
        "colab_type": "text"
      },
      "source": [
        "# **Assignment #4:**\n",
        "Modify the DNN to see if you can improve the accuracy and loss. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQy0TojyOkNM",
        "colab_type": "text"
      },
      "source": [
        "# **Assignment #5:** \n",
        "Train the DNN with the amazon reivews.<br>\n",
        "<br>\n",
        "Use different variable names than the ones used for the Yelp reviews. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x6ZEW6238GM",
        "colab_type": "text"
      },
      "source": [
        "# **Word Embedding**\n",
        "There are various ways to vectorize text, such as:\n",
        "*   Words represented as a vector.\n",
        "*   Characters represented as a vector\n",
        "\n",
        "\n",
        "In this notebook, you’ll see how to deal with representing words as vectors which is the common way to use text in neural networks. Two possible ways to represent a word as a vector are:\n",
        "*   one-hot encoding\n",
        "*   word embeddings<br>\n",
        "\n",
        "The first example shown below does one-hot encoding. The second example does word embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocESp_nW4Nff",
        "colab_type": "text"
      },
      "source": [
        "**Hot-one encoding Example**<br>\n",
        "In this example you experiment with one hot encoding on a small dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvmlZhoC3-X5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#There are five cities, vocabulary size of three\n",
        "food = ['bacon', 'egg', 'bacon', 'beer', 'toast']\n",
        "food"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bKNLRvaAXqH",
        "colab_type": "text"
      },
      "source": [
        "Convert the foods into one their one-hot equivalents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avkg8U2t4VWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "food_labels = encoder.fit_transform(food)\n",
        "\n",
        "#create a dataframe to examine the data\n",
        "df = pd.DataFrame()\n",
        "df['food']= food\n",
        "df['food_labels']= food_labels\n",
        "df.sort_values(by=['food'])\n",
        "\n",
        "#Convert the food into one-hot values\n",
        "cat_columns = [\"food_labels\"]\n",
        "df_processed = pd.get_dummies(df, prefix_sep=\"__\",columns=cat_columns)\n",
        "df_processed\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIzHxBvk4mez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Or use the OneHotEncoder method to encode the data\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "food_labels = food_labels.reshape((5, 1))\n",
        "encoder.fit_transform(food_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nrlu8m1jQVyw",
        "colab_type": "text"
      },
      "source": [
        "# **Assignment #6:**\n",
        "1. Create a list of 10 household appliances.\n",
        "2. Convert the list into one-hot encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZe8bcPNRNlz",
        "colab_type": "text"
      },
      "source": [
        "# **Discussion:** \n",
        "What are some downsides of one-hot encoding?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkYyfbnJjPS_",
        "colab_type": "text"
      },
      "source": [
        "# **Word embedding**<br>\n",
        "A word embedding is a learned representation for text where words that have the same meaning have a similar representation. It is this approach to representing words and documents that may be considered one of the key breakthroughs of deep learning on challenging natural language processing problems.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50ExQx2fbmh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "page(4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE096qUiyYPM",
        "colab_type": "text"
      },
      "source": [
        "Word embedding represents words as dense word vectors (also called word embeddings) which are trained unlike the one-hot encoding which are hardcoded. This means that the word embeddings collect more information into fewer dimensions.\n",
        "\n",
        "Note that the word embeddings do not understand the text as a human would, but they rather map the statistical structure of the language used in the corpus. Their aim is to map semantic meaning into a geometric space. This geometric space is then called the embedding space.<br>\n",
        "\n",
        "This would map semantically similar words close on the embedding space like numbers or colors. If the embedding captures the relationship between words well, things like vector arithmetic should become possible. A famous example in this field of study is the ability to map King - Man + Woman = Queen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0c2w2KtRnij",
        "colab_type": "text"
      },
      "source": [
        "Word embedding has fewer dimensions than one-hot encoding<br>\n",
        "Word embedding places similar words near each other<br>\n",
        "One-hot encoding has a sparse matrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9EAi8ZEyjnH",
        "colab_type": "text"
      },
      "source": [
        "There are two methods for doing word embedding: <br> \n",
        "\n",
        ">1.Train your word embeddings during the training of your neural network. <br>\n",
        ">2.Use pretrained word embeddings which you can directly use in your model. You can leave these word embeddings unchanged during training or you can train them.<br><br>\n",
        "\n",
        "Then tokenize the data into a format that can be used by the word embeddings. <br><br>\n",
        "Keras offers a couple of convenience methods for text preprocessing and sequence preprocessing which you can employ to prepare your text.<br>\n",
        "\n",
        "[Keras Tokenizer ](https://keras.io/preprocessing/text/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjmlLvVCyZF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "#Go through all the reviews and keep 3000 words.\n",
        "tokenizer = Tokenizer(num_words=3000) #keep 3000 words\n",
        "\n",
        "#Update the internal vocabulary based on a list of texts\n",
        "#Must be run before running texts_to_sequences\n",
        "tokenizer.fit_on_texts(sentences_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL4deq14GHCC",
        "colab_type": "text"
      },
      "source": [
        "The number assigned to each word is dependent upon is frequency of use in all the sentences. <br>\n",
        "For example:<br>\n",
        ">'the' is 1<br>\n",
        "'and' is 2<br>\n",
        "'was' is 3<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJhtYbGiFWwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Examples of reviews as word embeddings\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "print(sentences_train[3],X_train[3])\n",
        "print(sentences_train[23],X_train[23])\n",
        "print(sentences_train[620],X_train[620])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT22An4iFduH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "\n",
        "print(\"vocab size=\", vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXBx3rLC0IQb",
        "colab_type": "text"
      },
      "source": [
        "The indexing begins with the most common word first (the). <br>\n",
        "It is important to note that the index 0 is reserved and is not assigned to any word. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDkVfwDTzm0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Entering a word that is not in the texts will\n",
        "#generate an error\n",
        "for word in ['the', 'all', 'bad', 'terrible','horrible','lost','lukewarm','bacon']: \n",
        "    print('{}: {}'.format(word, tokenizer.word_index[word]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN5hojTdHTAq",
        "colab_type": "text"
      },
      "source": [
        "The list can be searched by word or by index. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaLJRwI9G2CS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#What is the least used word in the list? \n",
        "print((tokenizer.index_word[1746]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6F_ZYOCUUIZ",
        "colab_type": "text"
      },
      "source": [
        "# **Assignment #7:**\n",
        "Use the Amazon reviews to do word embedding. \n",
        "<br>\n",
        "Use different variable names than the ones used for the Yelp reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgJBNUDVtmlH",
        "colab_type": "text"
      },
      "source": [
        "# **Find similar words with gensim**<br>\n",
        "Gensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning. Gensim is implemented in Python and Cython."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne2MZqlUbjzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim.downloader as api\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
        "#Find words similiar to other words\n",
        "#If done correctly, you can do math with words\n",
        "result = word_vectors.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])\n",
        "print(\"{}: {:.4f}\".format(*result[0]))\n",
        "#Try sandwich, tuna, bread"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2jxdrUeU6Ap",
        "colab_type": "text"
      },
      "source": [
        "# **Assignment #8:** \n",
        "Use the gensim library to find other word equations. <br>\n",
        "Share them with the class. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPB6nRbJkUgQ",
        "colab_type": "text"
      },
      "source": [
        "# **Pad the sequence of words**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hZnB5SD1E82",
        "colab_type": "text"
      },
      "source": [
        "One problem that we have is that each text sequence has different number of words. To fix this, you can use pad_sequence() which simply pads the sequence of words with zeros. By default, it prepends zeros but we want to append them. Typically it does not matter whether you prepend or append zeros.\n",
        "\n",
        "Additionally you would want to add a maxlen parameter to specify how long the sequences should be. This cuts sequences that exceed that number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSmvBgUk1yRg",
        "colab_type": "text"
      },
      "source": [
        "The resulting feature vector contains mostly zeros, when you have a fairly short sentence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25FkOsNL1Jl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "#The maximum length of a review, cut off the extra words \n",
        "maxlen = 100\n",
        "#If a review is less than 100 words, pad the vector with 0s.\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "print(X_train.shape,X_test.shape)\n",
        "print(y_train.shape,y_test.shape, \"\\n\")\n",
        "\n",
        "index=5\n",
        "print(\"The review:\\n\",sentences_train[index])\n",
        "print(\"\\nThe final feature vector:\\n\",X_train[index, :])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF1tbYdmWr_Q",
        "colab_type": "text"
      },
      "source": [
        "# **Assignment #9:** \n",
        "Pad the vectors of the Amazon reviews.<br>\n",
        "Use different variable names than the ones used for the Yelp reviews "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t4ZMILvfWr9",
        "colab_type": "text"
      },
      "source": [
        "# **Train the Embedded Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ImkmiOQ1632",
        "colab_type": "text"
      },
      "source": [
        "Now you can use the Embedding Layer of Keras which takes the previously calculated integers and maps them to a dense vector of the embedding. <br>\n",
        "You will need the following parameters:<br>\n",
        "\n",
        ">input_dim: the size of the vocabulary<br>\n",
        "output_dim: the size of the dense vector<br>\n",
        "input_length: the length of the sequence<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsJiBvQbe7oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "page(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS8ud39NJsAn",
        "colab_type": "text"
      },
      "source": [
        "The output of the Embedding layer is a 2D vector with one embedding for each word in the input sequence of words (input document).\n",
        "\n",
        "If you wish to connect a Dense layer directly to an Embedding layer, you must first flatten the 2D output matrix to a 1D vector using the Flatten layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_A0dS0p17hu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"input dim=\",input_dim)\n",
        "print(\"output dim of embedding layer=\",embedding_dim)\n",
        "print(\"input length = \", maxlen)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuZ3mihe3I1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_train.shape,X_test.shape)\n",
        "print(y_train.shape,y_test.shape)\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv2-eUCdfKW1",
        "colab_type": "text"
      },
      "source": [
        "# **Assignment #10**: \n",
        "Train the embedded model using the Amazon reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdnxohmj4Bsy",
        "colab_type": "text"
      },
      "source": [
        "This is typically a not very reliable way to work with sequential data as you can see in the performance. When working with sequential data you want to focus on methods that look at local and sequential information instead of absolute positional information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh_f1H7AfoXI",
        "colab_type": "text"
      },
      "source": [
        "# **Use a MaxPooling Layer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxV9Z7lU4KDb",
        "colab_type": "text"
      },
      "source": [
        "Another way to work with embeddings is by using a MaxPooling1D/AveragePooling1D or a GlobalMaxPooling1D/GlobalAveragePooling1D layer after the embedding. You can think of the pooling layers as a way to downsample (a way to reduce the size of) the incoming feature vectors.\n",
        "\n",
        "In the case of max pooling you take the maximum value of all features in the pool for each feature dimension. In the case of average pooling you take the average, but max pooling seems to be more commonly used as it highlights large values.\n",
        "\n",
        "Global max/average pooling takes the maximum/average of all features whereas in the other case you have to define the pool size. Keras has again its own layer that you can add in the sequential model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIkoyLv-fqRf",
        "colab_type": "text"
      },
      "source": [
        "Global max pooling = ordinary max pooling layer with pool size equals to the size of the input.<br>\n",
        "\n",
        "Advantages of Global Pooling:\n",
        "* it is more native to the convolution structure by enforcing correspondences between feature maps and categories.\n",
        "* there is no parameter to optimize in the global average pooling thus overfitting is avoided at this layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvcohC3N4NCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MU5Xh1l4Tjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7_Y_6-AgLMj",
        "colab_type": "text"
      },
      "source": [
        "# **Assignment #11:** \n",
        "Use the Amazon dataset to train the model with a max pooling layer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkM6CvrglKtG",
        "colab_type": "text"
      },
      "source": [
        "# **Use a precomputed embedding space**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1S2XEG15FNr",
        "colab_type": "text"
      },
      "source": [
        "Can performance be improved using a precomputed embedding space that utilizes a much larger corpus? <br>\n",
        "It is possible to precompute word embeddings by simply training them on a large corpus of text. Among the most popular methods are Word2Vec developed by Google and GloVe (Global Vectors for Word Representation) developed by the Stanford NLP Group.<br>\n",
        "\n",
        "Word2Vec achieves this by employing neural networks and GloVe achieves this with a co-occurrence matrix and by using matrix factorization. <br>\n",
        "In both cases you are dealing with dimensionality reduction:  <br>\n",
        ">Word2Vec is more accurate  <br>\n",
        "GloVe is faster to compute.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDHM_LB98qW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yg52fWY8vrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "embedding_dim = 50\n",
        "embedding_matrix = create_embedding_matrix(\n",
        "    '/gdrive/My Drive/wiki-news-300d-1M.vec',\n",
        "    tokenizer.word_index, embedding_dim)\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zrIyzaHZmHC",
        "colab_type": "text"
      },
      "source": [
        "Percentage of vocabulary covered by the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zerTRtUWZhbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
        "nonzero_elements / vocab_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZhhgaTJQgHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to see the missing words: \n",
        "t = np.count_nonzero(embedding_matrix, axis=1)\n",
        "\n",
        "count = len(t)\n",
        "missing_words =[]\n",
        "for i in range(1,count):\n",
        "  if t[i] == 0:\n",
        "    missing_words.append(tokenizer.index_word[i])\n",
        "\n",
        "missing_words[10:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVbEXqtepJk0",
        "colab_type": "text"
      },
      "source": [
        "# **Trial 3: Embedded DNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FexrZH7hROlC",
        "colab_type": "text"
      },
      "source": [
        "In this trial use the pretrained embedded layer and the larger corpus of words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqjXj7jZZts8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=maxlen, \n",
        "                           trainable=False)) #use pretrained\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRcByh6-Zycf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AgZhVA8RdbX",
        "colab_type": "text"
      },
      "source": [
        "In this trial use the larger corpus of words and train the embedded layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTgLVaxeZ8S5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=maxlen, \n",
        "                           trainable=True)) #train this layer\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rgHBRNDaAYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrmOUG3jSB6k",
        "colab_type": "text"
      },
      "source": [
        "# **Convolututional Neural Network (CNN)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT3MaMvPacWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVPXIiu8agJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWm19xItSY19",
        "colab_type": "text"
      },
      "source": [
        "You can see the 80% accuracy of the Logistic Regression model seems  tough ot beat with this data set and a CNN might not be well equipped. <br>\n",
        "The reason for such a plateau might be that:\n",
        "1. There are not enough training samples\n",
        "2. The data you have does not generalize well\n",
        "3. Missing focus on tweaking the hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wR2yiXOvTL0v",
        "colab_type": "text"
      },
      "source": [
        "# **HyperParameter Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os0yrNZaapZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
        "    model = Sequential()\n",
        "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(10, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',metrics=[\"acc\"])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLEXBJP4ktau",
        "colab_type": "text"
      },
      "source": [
        "**Embedding dimension**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLd-_2jQasgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_grid = dict(num_filters=[32, 64, 128],\n",
        "                  kernel_size=[3, 5, 7],\n",
        "                  vocab_size=[5000], \n",
        "                  embedding_dim=[50],\n",
        "                  maxlen=[100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dW84sk_fdT2",
        "colab_type": "text"
      },
      "source": [
        "# **HyperParameter Grid Search of each text set**\n",
        "Perform the random search method of hyperparameter tuning to improve the model performance. <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlkYfUuKaww4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Main settings\n",
        "epochs = 20\n",
        "embedding_dim = 50\n",
        "maxlen = 100\n",
        "output_file = '/gdrive/My Drive/output.txt'\n",
        "\n",
        "# Run grid search for each source (yelp, amazon, imdb)\n",
        "for source, frame in df.groupby('source'):\n",
        "    print('Running grid search for data set :', source)\n",
        "    sentences = df['sentence'].values\n",
        "    y = df['label'].values\n",
        "\n",
        "    # Train-test split\n",
        "    sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
        "        sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "    # Tokenize words\n",
        "    tokenizer = Tokenizer(num_words=5000)\n",
        "    tokenizer.fit_on_texts(sentences_train)\n",
        "    X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "    X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "    # Adding 1 because of reserved 0 index\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "    # Pad sequences with zeros\n",
        "    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "    X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "    # Parameter grid for grid search\n",
        "    param_grid = dict(num_filters=[32, 64, 128],\n",
        "                      kernel_size=[3, 5, 7],\n",
        "                      vocab_size=[vocab_size],\n",
        "                      embedding_dim=[embedding_dim],\n",
        "                      maxlen=[maxlen])\n",
        "    model = KerasClassifier(build_fn=create_model,\n",
        "                            epochs=epochs, batch_size=10,\n",
        "                            verbose=False)\n",
        "    grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
        "                              cv=4, verbose=1, n_iter=5)\n",
        "    grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate testing set\n",
        "    test_accuracy = grid.score(X_test, y_test)\n",
        "\n",
        "    # Save and evaluate results\n",
        "    with open(output_file, 'a') as f:\n",
        "        s = ('Running {} data set\\nBest Accuracy : '\n",
        "             '{:.4f}\\n{}\\nTest Accuracy : {:.4f}\\n\\n')\n",
        "        output_string = s.format(\n",
        "            source,\n",
        "            grid_result.best_score_,\n",
        "            grid_result.best_params_,\n",
        "            test_accuracy)\n",
        "        print(output_string)\n",
        "        f.write(output_string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfvbZPSoTU3j",
        "colab_type": "text"
      },
      "source": [
        "# **HyperParameter Tuning on all the datasets together**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ShY0OgEZqPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a dataframe containing all three sources\n",
        "filepath_dict = {'yelp':   'yelp_labelled.txt',\n",
        "                 'amazon': 'amazon_cells_labelled.txt',\n",
        "                 'imdb':   'imdb_labelled.txt'}\n",
        "\n",
        "df_full_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df_full = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "    df_full['source'] = source  # Add another column filled with the source name\n",
        "    df_full_list.append(df)\n",
        "\n",
        "df_full = pd.concat(df_list)\n",
        "print(df_full.iloc[2000])\n",
        "print(\"dataframe shape: \",df_full.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mm86EImSbLBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Main settings\n",
        "epochs = 20\n",
        "embedding_dim = 50\n",
        "maxlen = 100\n",
        "output_file = '/gdrive/My Drive/output.txt'\n",
        "\n",
        "print('Running grid search for data set :\\n', df_full)\n",
        "sentences = df['sentence'].values\n",
        "y = df['label'].values\n",
        "\n",
        "# Train-test split\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
        "    sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "# Tokenize words\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "# Adding 1 because of reserved 0 index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Pad sequences with zeros\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Parameter grid for grid search\n",
        "param_grid = dict(num_filters=[32, 64, 128],\n",
        "                      kernel_size=[3, 5, 7],\n",
        "                      vocab_size=[vocab_size],\n",
        "                      embedding_dim=[embedding_dim],\n",
        "                      maxlen=[maxlen])\n",
        "model = KerasClassifier(build_fn=create_model,\n",
        "                            epochs=epochs, batch_size=10,\n",
        "                            verbose=False)\n",
        "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
        "                              cv=4, verbose=1, n_iter=5)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate testing set\n",
        "test_accuracy = grid.score(X_test, y_test)\n",
        "\n",
        "# Save and evaluate results\n",
        "with open(output_file, 'a') as f:\n",
        "   s = ('Running {} data set\\nBest Accuracy : '\n",
        "        '{:.4f}\\n{}\\nTest Accuracy : {:.4f}\\n\\n')\n",
        "   output_string = s.format(\n",
        "            source,\n",
        "            grid_result.best_score_,\n",
        "            grid_result.best_params_,\n",
        "            test_accuracy)\n",
        "   print(output_string)\n",
        "   f.write(output_string)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}