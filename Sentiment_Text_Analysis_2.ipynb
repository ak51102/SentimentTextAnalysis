{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Text Analysis 2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "mount_file_id": "1aidmupgYxzzx8z-mYF9gO-zivBJUsMBS",
      "authorship_tag": "ABX9TyMgQeUBQiEwH4JU9CS18Dsv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/SentimentTextAnalysis/blob/master/Sentiment_Text_Analysis_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a68Nq4HgJDF",
        "colab_type": "text"
      },
      "source": [
        "# **Sentiment Text Analyis**<br>\n",
        "This notebook demonstrates different machine learning methods for determing if text reviews are positive or negative. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43caXH-JktTN",
        "colab_type": "text"
      },
      "source": [
        "# **Pre-requisites:**<br>\n",
        "Python - 2 day course is sufficient<br>\n",
        "Keras understanding - intro course is sufficient<br>\n",
        "logistic regression - understanding<br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPxnK5iGdOO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone  https://github.com/cagBRT/SentimentTextAnalysis.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP-R61seyu6s",
        "colab_type": "text"
      },
      "source": [
        "# **Pre-work**\n",
        "**Do this step before coming to class. It may take as long as 30 minutes to complete the download and upload.**<br>\n",
        "<br>\n",
        "You will need to add a file to your google drive. <br>\n",
        "1. Download the file to your computer. \n",
        ">Click on the link below. Then click **Download** <br>\n",
        "The download can take as long as 15 minutes.<br>\n",
        "The file to download is: [fileToAddToGoogleDrive](https://drive.google.com/open?id=1zJI1Xz-CgaQqX1UtBcOhUjEKWcSt6QK6)<br>\n",
        "The file is large: 2GBytes<br><br>\n",
        "\n",
        "\n",
        "2. Upload the file to Google Drive:<br>\n",
        ">Open Google Drive<br>\n",
        "On the Drive menu, click on **New** >> **File Upload**<br>\n",
        "Find the file on your computer, click on it and upload the file. \n",
        "\n",
        "The file is large, it may take as long as 15 minutes<br>\n",
        "Once the file is on you<br><br>\n",
        "The file is from a website: [English word vectors](https://fasttext.cc/docs/en/english-vectors.html)<br>\n",
        "This page gathers several pre-trained word vectors trained using fastText."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GUmmVaSCDTV",
        "colab_type": "text"
      },
      "source": [
        "# **Mount your Google Drive on this CoLab Notebook**\n",
        "Execute the following code cell<br>\n",
        "Click on the given link<br>\n",
        "Select your user name<br>\n",
        "Click **Allow**<br>\n",
        "Copy the authorization code<br>\n",
        "Paste the authorization code into the user input box. <br>\n",
        "You Google Drive is mounted to this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-0Y2vgPCDfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C_h3O7HD08K",
        "colab_type": "text"
      },
      "source": [
        "# **Check that your drive is mounted**\n",
        "1. On the menu bar, click the **folder icon**<br>\n",
        "2. Click on the **folder icon with the up arrow**\n",
        "3. Click on **gdrive**\n",
        "4. Click on **My Drive**\n",
        "5. Check for the file called **wiki-news-300d-1M.vec **<br>\n",
        "If the file is there, you have correctly installed the necessary files for this notebook. <br>\n",
        "If the file is not there, follow the instructions and try again. IF you are still having trouble email me: <br>\n",
        ">catherine.gamboa@bluerivert.com\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4fFT8ZFeTDT",
        "colab_type": "text"
      },
      "source": [
        "This notebook uses the [scikit-learn](https://scikit-learn.org/stable/) library<br>\n",
        "Scikit-learn is a free software machine learning library for Python.<br>\n",
        "*   Simple and efficient tools for predictive data analysis\n",
        "*  Accessible to everybody, and reusable in various contexts\n",
        "*Built on NumPy, SciPy, and matplotlib\n",
        "*Open source, commercially usable - BSD license\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTXaGN8jfvT3",
        "colab_type": "text"
      },
      "source": [
        "# **Import the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6UfoWoDombL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ_ITPKgr0xj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caQfFnw6lU6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs6VDuowfVAg",
        "colab_type": "text"
      },
      "source": [
        "# **Examine the data**<br>\n",
        "The data is from three sources: <br>\n",
        "> yelp reviews<br>\n",
        "> amazon reviews<br>\n",
        "> movie reviews<br>\n",
        "\n",
        "The data has the structure: <br>\n",
        ">\"review text\" label source<br>\n",
        "\n",
        "**review text is called**: sentence<br>\n",
        "**label**: 0 = negative review, 1 = positive review<br>\n",
        "**source**: yelp, amazon, imdb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TlFVDiLdnIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!cat yelp_labelled.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z3rmOeecdQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a dataframe containing all three sources\n",
        "filepath_dict = {'yelp':   'yelp_labelled.txt',\n",
        "                 'amazon': 'amazon_cells_labelled.txt',\n",
        "                 'imdb':   'imdb_labelled.txt'}\n",
        "\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "    df['source'] = source  # Add another column filled with the source name\n",
        "    df_list.append(df)\n",
        "\n",
        "df = pd.concat(df_list)\n",
        "print(df.iloc[0])\n",
        "print(\"dataframe shape: \",df.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otrPFs_FfJOB",
        "colab_type": "text"
      },
      "source": [
        "# **Create a Bag of Words**\n",
        "The bag-of-words model is a word/sentence representation used in NLP and information retrieval. <br>\n",
        "In this model, text is represented as a bag of its words, *disregarding grammar and even word order*.\n",
        "\n",
        "Create a bag of words (BoW) for vectorizing the text. <br>\n",
        "Take the data and create a vocabulary from all the words in all reviews.<br>\n",
        "The collection of texts is also called a **corpus** in NLP.<br>\n",
        "The **vocabulary** in this case is a list of words that occurred in our text where each word has its own index.<br>\n",
        "The resulting vector is also called a **feature vector**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSzPfHZZhHJS",
        "colab_type": "text"
      },
      "source": [
        "# **Bag of Words Example:** <br>\n",
        "# **Create a bag of words (BoW) for vectorizing the text**\n",
        "\n",
        "Create a toy set of words to demonstrate BoW.<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7VqhzIZd_hG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1. The words\n",
        "john_words = ['Sam likes to run.', \n",
        "              'John hates to be cold.', \n",
        "              'John hates to be late.'\n",
        "            ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5UPXWFMeFTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2. Create the bag of words\n",
        "#The words are in alphabetical order (uppercase listed before lowercase)\n",
        "#Each word is assigned a number\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
        "vectorizer.fit(john_words)\n",
        "vocab = vectorizer.vocabulary_\n",
        "vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM-AeTLVewzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#3. Convert the sentences in to vectors\n",
        "#each word used in a sentence has a '1' in the corresponding column\n",
        "dfbow = pd.DataFrame()\n",
        "dfbow['sentences'] = john_words\n",
        "vector = vectorizer.transform(john_words).toarray().tolist()\n",
        "dfbow['vector'] = vector\n",
        "print(vocab)\n",
        "dfbow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_obQxnIey13Q",
        "colab_type": "text"
      },
      "source": [
        "Notice in the above output: \n",
        "1. All the words in all the sentences created the vocabulary.<br>\n",
        "2. Each sentence is turned into a vector. <br>\n",
        "3. The length of the vector is the length of the vocabulary.<br>\n",
        "4. A '1' in the vector indicates the word is used in the sentence, a '0' means it is not used in the sentence. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vHNtHmOL0lQ",
        "colab_type": "text"
      },
      "source": [
        "# **Assignment 1**: <br>\n",
        "1. Write 5 sentences. \n",
        "2. Create a BoW from the sentences. \n",
        "3. Vectorize the sentences. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPM1uU2mf-15",
        "colab_type": "text"
      },
      "source": [
        "# **Split the review data into train and test sets**\n",
        "\n",
        "Split the Yelp data into training and tests sets<br>\n",
        "\n",
        "[train_test_split](https://www.bitdegree.org/learn/train-test-split)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6jCeLe9f8pp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#select the rows of the data set that are from yelp\n",
        "df_yelp = df[df['source'] == 'yelp']\n",
        "\n",
        "sentences = df_yelp['sentence'].values\n",
        "y = df_yelp['label'].values\n",
        "\n",
        "#do a 75 - 25 split between train and test data\n",
        "#If int, random_state is the seed used by the random number generator; \n",
        "#If RandomState instance, random_state is the random number generator; \n",
        "#If None, the random number generator is the RandomState instance used by np.random.\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
        "   sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "#print out the first sentence of the training set\n",
        "print(sentences_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT0D7qx5gPtD",
        "colab_type": "text"
      },
      "source": [
        "# **Vectorize the training and test set**\n",
        "Vectorize the data: <br>\n",
        "\n",
        "Assign each word a number.<br>\n",
        "Count the number of times each word appears in the individual review text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrXmpO--gP4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#1. use the words from the training set\n",
        "#2. create a BoW from the yelp reviews\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(sentences_train)\n",
        "vocab = vectorizer.vocabulary_\n",
        "vocab = pd.Series(vocab)\n",
        "#2. vectorize the sentences\n",
        "X_train = vectorizer.transform(sentences_train)\n",
        "X_test  = vectorizer.transform(sentences_test)\n",
        "print(\"done\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE6MTaQNJXh5",
        "colab_type": "text"
      },
      "source": [
        "What has been done so far: \n",
        "1. Created a vocabulary from all the words used in the yelp reviews.\n",
        "2. Assigned each word a number.<br>\n",
        "\n",
        "Now check the vectorization of the sentences in the yelp review training and test data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GFHSK8qJDAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\"Select a number between 0 - 749\n",
        "check=24\n",
        "print(sentences_train[check])\n",
        "print(X_train[check])\n",
        "#Prints sentence number, word vector, quantity of word in sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIn6nF381OLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\"Select a number between 0 - 749\n",
        "check=24\n",
        "print(sentences_test[check])\n",
        "print(X_test[check])\n",
        "#Prints sentence number, word vector, quantity of word in sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svvE2Vfogj4Z",
        "colab_type": "text"
      },
      "source": [
        "**The training set has:** <br>\n",
        "750 examples<br>\n",
        "1714 words in the vocabulary<br>\n",
        "\n",
        "It is a sparse matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRTOQFRqMdbX",
        "colab_type": "text"
      },
      "source": [
        "# **Assignment #2**: \n",
        "1. Split the Amazon reviews into a training and test set.\n",
        "2. Create a BoW using the amazon training and test reviews. \n",
        "2. Print out the vectorization of two or three reviews from the test and training set. <br>\n",
        "<br>\n",
        "\n",
        "**Make sure your variable names for the Amazon BoW are different than the Yelp BoW.** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ8WySaNjHQg",
        "colab_type": "text"
      },
      "source": [
        "# **Create a logistic regression model and train it**<br>\n",
        "\n",
        "The [Logistic Regression class](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) from Scikit Learn implements regularized logistic regression. It can handle both dense and sparse input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXYDqEdrjHar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n",
        "score = classifier.score(X_test, y_test)\n",
        "\n",
        "print(\"Accuracy:\", score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4tRFkQBjhR1",
        "colab_type": "text"
      },
      "source": [
        "# **Assignment 3:**\n",
        "\n",
        "Perform logistic regression on all three data sets<br>\n",
        "yelp<br>\n",
        "amazon<br>\n",
        "imdb<br>\n",
        "\n",
        "Get a baseline using logistic regression. This will give us something to compare with the other methods. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y74YPIkbjYMu",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title \n",
        "for source in df['source'].unique():\n",
        "    df_source = df[df['source'] == source]\n",
        "    reviews = df_source['sentence'].values\n",
        "    reviews_y = df_source['label'].values\n",
        "\n",
        "    reviews_train, reviews_test, reviews_y_train, reviews_y_test = train_test_split(\n",
        "        reviews, reviews_y, test_size=0.25, random_state=1000)\n",
        "\n",
        "    vectorizer = CountVectorizer()\n",
        "    vectorizer.fit(reviews_train)\n",
        "    r_X_train = vectorizer.transform(reviews_train)\n",
        "    r_X_test  = vectorizer.transform(reviews_test)\n",
        "\n",
        "    classifier = LogisticRegression()\n",
        "    classifier.fit(r_X_train, reviews_y_train)\n",
        "    score = classifier.score(r_X_test, reviews_y_test)\n",
        "    print('Accuracy for {} data: {:.4f}'.format(source, score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWkguVMWlc3b",
        "colab_type": "text"
      },
      "source": [
        "# **Trial 1:Keras DNN**\n",
        "Create a DNN using Keras, use the bag of words.<br> \n",
        "Compare it to the logistic regession using the same data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl_GA_keldAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = X_train.shape[1]  # Number of features\n",
        "print(\"model imputs = \", input_dim)\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(1700, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(1000, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpfbM8Plr_6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG0lvs-h7CkW",
        "colab_type": "text"
      },
      "source": [
        "**Train the DNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MClsUloHsfeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=7,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNfZaUNtssWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4YFCVmuszj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geAPUYZE-Ddt",
        "colab_type": "text"
      },
      "source": [
        "The Deep Neural Network trained using Bag of Words is overfit. <br>\n",
        "Play with the model architecture and hyperparameters to see if you can find a better model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRNL-Xsls22X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdMV7FP8-Yob",
        "colab_type": "text"
      },
      "source": [
        "Text is considered a form of sequence data similar to time series data that you would have in weather data or financial data. <br>\n",
        "In the BOW model, you  represented an entire review as a single feature vector. In the next section, each word is represented as a vector. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x6ZEW6238GM",
        "colab_type": "text"
      },
      "source": [
        "# **Word Embedding**\n",
        "There are various ways to vectorize text, such as:\n",
        "*   Words represented as a vector.\n",
        "*   Characters represented as a vector\n",
        "\n",
        "\n",
        "In this notebook, you’ll see how to deal with representing words as vectors which is the common way to use text in neural networks. Two possible ways to represent a word as a vector are:\n",
        "*   one-hot encoding\n",
        "*   word embeddings<br>\n",
        "\n",
        "The first example shown below does one-hot encoding. The second example does word embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocESp_nW4Nff",
        "colab_type": "text"
      },
      "source": [
        "**Hot-one encoding Example**<br>\n",
        "In this example you experiment with one hot encoding on a small dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvmlZhoC3-X5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#There are five cities, vocabulary size of three\n",
        "food = ['bacon', 'egg', 'bacon', 'beer', 'toast']\n",
        "food"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bKNLRvaAXqH",
        "colab_type": "text"
      },
      "source": [
        "Convert the cities into one their one-hot equivalents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avkg8U2t4VWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "food_labels = encoder.fit_transform(cities)\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['food']= food\n",
        "df['food_labels']= food_labels\n",
        "df.sort_values(by=['food'])\n",
        "\n",
        "cat_columns = [\"food_labels\"]\n",
        "df_processed = pd.get_dummies(df, prefix_sep=\"__\",columns=cat_columns)\n",
        "df_processed\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIzHxBvk4mez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "city_labels = city_labels.reshape((5, 1))\n",
        "encoder.fit_transform(city_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkYyfbnJjPS_",
        "colab_type": "text"
      },
      "source": [
        "**Word embedding**<br>\n",
        "Word embedding has fewer dimensions than one-hot encoding<br>\n",
        "Word embedding places similar words near each other<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE096qUiyYPM",
        "colab_type": "text"
      },
      "source": [
        "Word embedding represents words as dense word vectors (also called word embeddings) which are trained unlike the one-hot encoding which are hardcoded. This means that the word embeddings collect more information into fewer dimensions.\n",
        "\n",
        "Note that the word embeddings do not understand the text as a human would, but they rather map the statistical structure of the language used in the corpus. Their aim is to map semantic meaning into a geometric space. This geometric space is then called the embedding space.<br>\n",
        "\n",
        "This would map semantically similar words close on the embedding space like numbers or colors. If the embedding captures the relationship between words well, things like vector arithmetic should become possible. A famous example in this field of study is the ability to map King - Man + Woman = Queen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9EAi8ZEyjnH",
        "colab_type": "text"
      },
      "source": [
        "How can you get such a word embedding? <br>\n",
        "You have two options for this. \n",
        "\n",
        ">1.Train your word embeddings during the training of your neural network. <br>\n",
        ">2.Use pretrained word embeddings which you can directly use in your model. You can leave these word embeddings unchanged during training or you can train them.<br><br>\n",
        "\n",
        "Then tokenize the data into a format that can be used by the word embeddings. <br><br>\n",
        "Keras offers a couple of convenience methods for text preprocessing and sequence preprocessing which you can employ to prepare your text.<br>\n",
        "\n",
        "[Keras Tokenizer ](https://keras.io/preprocessing/text/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjmlLvVCyZF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "#Go through all the reviews and keep only 3000 of them.\n",
        "tokenizer = Tokenizer(num_words=3000) #keep 3000 words\n",
        "\n",
        "#Update the internal vocabulary based on a list of texts\n",
        "#Must be run before running texts_to_sequences\n",
        "t = tokenizer.fit_on_texts(sentences_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL4deq14GHCC",
        "colab_type": "text"
      },
      "source": [
        "The number assigned to each word is dependent upon is frequency of use in all the sentences. <br>\n",
        "For exampple:<br>\n",
        ">'the' is 1<br>\n",
        "'and' is 2<br>\n",
        "'was' is 3<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJhtYbGiFWwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Examples of reviews as word embeddings\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "print(sentences_train[3],X_train[3])\n",
        "print(sentences_train[23],X_train[23])\n",
        "print(sentences_train[620],X_train[620])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT22An4iFduH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "\n",
        "print(\"vocab size=\", vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXBx3rLC0IQb",
        "colab_type": "text"
      },
      "source": [
        "The indexing begins with the most common word first (the). <br>\n",
        "It is important to note that the index 0 is reserved and is not assigned to any word. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDkVfwDTzm0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Entering a word that is not in the texts will\n",
        "#generate an error\n",
        "for word in ['the', 'all', 'bad', 'terrible','horrible','lost','lukewarm','bacon']: \n",
        "    print('{}: {}'.format(word, tokenizer.word_index[word]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN5hojTdHTAq",
        "colab_type": "text"
      },
      "source": [
        "The list can be searched by word or by index. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaLJRwI9G2CS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#What is the least used word in the list? \n",
        "print((tokenizer.index_word[1746]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgJBNUDVtmlH",
        "colab_type": "text"
      },
      "source": [
        "**Find similar words with gensim**<br>\n",
        "Gensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning. Gensim is implemented in Python and Cython."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne2MZqlUbjzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim.downloader as api\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
        "#Find words similiar to other words\n",
        "#If done correctly, you can do math with numbers\n",
        "result = word_vectors.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])\n",
        "print(\"{}: {:.4f}\".format(*result[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPB6nRbJkUgQ",
        "colab_type": "text"
      },
      "source": [
        "**Pad the sequence of words**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hZnB5SD1E82",
        "colab_type": "text"
      },
      "source": [
        "One problem that we have is that each text sequence has in most cases different length of words. To counter this, you can use pad_sequence() which simply pads the sequence of words with zeros. By default, it prepends zeros but we want to append them. Typically it does not matter whether you prepend or append zeros.\n",
        "\n",
        "Additionally you would want to add a maxlen parameter to specify how long the sequences should be. This cuts sequences that exceed that number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSmvBgUk1yRg",
        "colab_type": "text"
      },
      "source": [
        "The resulting feature vector contains mostly zeros, when you have a fairly short sentence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25FkOsNL1Jl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "#The maximum length of a review, cut off the extra words \n",
        "maxlen = 100\n",
        "#If a review is less than 100 words, pad the vector with 0s.\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "print(X_train.shape,X_test.shape)\n",
        "print(y_train.shape,y_test.shape, \"\\n\")\n",
        "\n",
        "index=5\n",
        "print(sentences_train[index])\n",
        "print(X_train[index, :])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ImkmiOQ1632",
        "colab_type": "text"
      },
      "source": [
        "Now you can use the Embedding Layer of Keras which takes the previously calculated integers and maps them to a dense vector of the embedding. <br>\n",
        "You will need the following parameters:<br>\n",
        "\n",
        ">input_dim: the size of the vocabulary<br>\n",
        "output_dim: the size of the dense vector<br>\n",
        "input_length: the length of the sequence<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS8ud39NJsAn",
        "colab_type": "text"
      },
      "source": [
        "The output of the Embedding layer is a 2D vector with one embedding for each word in the input sequence of words (input document).\n",
        "\n",
        "If you wish to connect a Dense layer directly to an Embedding layer, you must first flatten the 2D output matrix to a 1D vector using the Flatten layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_A0dS0p17hu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"input dim=\",input_dim)\n",
        "print(\"output dim of embedding layer=\",embedding_dim)\n",
        "print(\"input length = \", maxlen)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuZ3mihe3I1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_train.shape,X_test.shape)\n",
        "print(y_train.shape,y_test.shape)\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdnxohmj4Bsy",
        "colab_type": "text"
      },
      "source": [
        "This is typically a not very reliable way to work with sequential data as you can see in the performance. When working with sequential data you want to focus on methods that look at local and sequential information instead of absolute positional information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxV9Z7lU4KDb",
        "colab_type": "text"
      },
      "source": [
        "Another way to work with embeddings is by using a MaxPooling1D/AveragePooling1D or a GlobalMaxPooling1D/GlobalAveragePooling1D layer after the embedding. You can think of the pooling layers as a way to downsample (a way to reduce the size of) the incoming feature vectors.\n",
        "\n",
        "In the case of max pooling you take the maximum value of all features in the pool for each feature dimension. In the case of average pooling you take the average, but max pooling seems to be more commonly used as it highlights large values.\n",
        "\n",
        "Global max/average pooling takes the maximum/average of all features whereas in the other case you have to define the pool size. Keras has again its own layer that you can add in the sequential model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvcohC3N4NCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MU5Xh1l4Tjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1S2XEG15FNr",
        "colab_type": "text"
      },
      "source": [
        "use a precomputed embedding space that utilizes a much larger corpus. It is possible to precompute word embeddings by simply training them on a large corpus of text. Among the most popular methods are Word2Vec developed by Google and GloVe (Global Vectors for Word Representation) developed by the Stanford NLP Group.<br>\n",
        "\n",
        "Word2Vec achieves this by employing neural networks and GloVe achieves this with a co-occurrence matrix and by using matrix factorization. In both cases you are dealing with dimensionality reduction, but Word2Vec is more accurate and GloVe is faster to compute.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDHM_LB98qW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-MvYxlUBEso",
        "colab_type": "text"
      },
      "source": [
        "# **CHECK: DOES THE FOLLOWING CODE CELL EXECUTE WITHOUT ERRORS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXHAKUuDczvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yg52fWY8vrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "embedding_dim = 50\n",
        "embedding_matrix = create_embedding_matrix(\n",
        "    '/gdrive/My Drive/wiki-news-300d-1M.vec',\n",
        "    tokenizer.word_index, embedding_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zrIyzaHZmHC",
        "colab_type": "text"
      },
      "source": [
        "Percentage of vocabulary covered by the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zerTRtUWZhbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
        "nonzero_elements / vocab_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVbEXqtepJk0",
        "colab_type": "text"
      },
      "source": [
        "# **Trial 3: Embedded DNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqjXj7jZZts8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=maxlen, \n",
        "                           trainable=False))\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRcByh6-Zycf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTgLVaxeZ8S5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=maxlen, \n",
        "                           trainable=True))\n",
        "model.add(layers.GlobalMaxPool1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rgHBRNDaAYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=50,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT3MaMvPacWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVPXIiu8agJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=False,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os0yrNZaapZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
        "    model = Sequential()\n",
        "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(10, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',metrics=[\"acc\"])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLEXBJP4ktau",
        "colab_type": "text"
      },
      "source": [
        "**Embedding dimension**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLd-_2jQasgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_grid = dict(num_filters=[32, 64, 128],\n",
        "                  kernel_size=[3, 5, 7],\n",
        "                  vocab_size=[5000], \n",
        "                  embedding_dim=[50],\n",
        "                  maxlen=[100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dW84sk_fdT2",
        "colab_type": "text"
      },
      "source": [
        "# **HyperParameter Grid Search of each text set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoQUA3AVBjAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath_dict = {'yelp':   'yelp_labelled.txt',\n",
        "                 'amazon': 'amazon_cells_labelled.txt',\n",
        "                 'imdb':   'imdb_labelled.txt'}\n",
        "\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "    df['source'] = source  # Add another column filled with the source name\n",
        "    df_list.append(df)\n",
        "\n",
        "df = pd.concat(df_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlkYfUuKaww4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Main settings\n",
        "epochs = 20\n",
        "embedding_dim = 50\n",
        "maxlen = 100\n",
        "output_file = '/content/drive/My Drive/output.txt'\n",
        "\n",
        "# Run grid search for each source (yelp, amazon, imdb)\n",
        "for source, frame in df.groupby('source'):\n",
        "    print('Running grid search for data set :', source)\n",
        "    sentences = df['sentence'].values\n",
        "    y = df['label'].values\n",
        "\n",
        "    # Train-test split\n",
        "    sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
        "        sentences, y, test_size=0.25, random_state=1000)\n",
        "\n",
        "    # Tokenize words\n",
        "    tokenizer = Tokenizer(num_words=5000)\n",
        "    tokenizer.fit_on_texts(sentences_train)\n",
        "    X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "    X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "\n",
        "    # Adding 1 because of reserved 0 index\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "    # Pad sequences with zeros\n",
        "    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "    X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "    # Parameter grid for grid search\n",
        "    param_grid = dict(num_filters=[32, 64, 128],\n",
        "                      kernel_size=[3, 5, 7],\n",
        "                      vocab_size=[vocab_size],\n",
        "                      embedding_dim=[embedding_dim],\n",
        "                      maxlen=[maxlen])\n",
        "    model = KerasClassifier(build_fn=create_model,\n",
        "                            epochs=epochs, batch_size=10,\n",
        "                            verbose=False)\n",
        "    grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
        "                              cv=4, verbose=1, n_iter=5)\n",
        "    grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate testing set\n",
        "    test_accuracy = grid.score(X_test, y_test)\n",
        "\n",
        "    # Save and evaluate results\n",
        "    with open(output_file, 'a') as f:\n",
        "        s = ('Running {} data set\\nBest Accuracy : '\n",
        "             '{:.4f}\\n{}\\nTest Accuracy : {:.4f}\\n\\n')\n",
        "        output_string = s.format(\n",
        "            source,\n",
        "            grid_result.best_score_,\n",
        "            grid_result.best_params_,\n",
        "            test_accuracy)\n",
        "        print(output_string)\n",
        "        f.write(output_string)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}